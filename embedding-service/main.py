"""
Embedding API Service
μ„λ² λ”© μƒμ„± μ „μ© λ§μ΄ν¬λ΅μ„λΉ„μ¤
"""
from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from typing import List
from sentence_transformers import SentenceTransformer
import numpy as np
import logging

# Logging μ„¤μ •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI μ•±
app = FastAPI(
    title="Embedding Service",
    description="Text Embedding Generation Service",
    version="1.0.0"
)

# μ „μ—­ λ¨λΈ (μ‹±κΈ€ν†¤)
model = None
MODEL_NAME = "jhgan/ko-sroberta-multitask"


def get_model():
    """λ¨λΈ λ΅λ“ (lazy loading)"""
    global model
    if model is None:
        logger.info(f"Loading model: {MODEL_NAME}")
        model = SentenceTransformer(MODEL_NAME)
        logger.info("Model loaded successfully!")
    return model


# Request/Response μ¤ν‚¤λ§
class EmbeddingRequest(BaseModel):
    """μ„λ² λ”© μ”μ²­"""
    text: str


class BatchEmbeddingRequest(BaseModel):
    """λ°°μΉ μ„λ² λ”© μ”μ²­"""
    texts: List[str]


class EmbeddingResponse(BaseModel):
    """μ„λ² λ”© μ‘λ‹µ"""
    embedding: List[float]
    dimension: int


class BatchEmbeddingResponse(BaseModel):
    """λ°°μΉ μ„λ² λ”© μ‘λ‹µ"""
    embeddings: List[List[float]]
    count: int
    dimension: int


@app.on_event("startup")
async def startup_event():
    """μ‹μ‘ μ‹ λ¨λΈ λ―Έλ¦¬ λ΅λ“"""
    logger.info("π€ Starting Embedding Service...")
    try:
        get_model()
        logger.info("β… Model pre-loaded successfully!")
    except Exception as e:
        logger.error(f"β Error loading model: {e}")


@app.get("/")
async def root():
    """λ£¨νΈ μ—”λ“ν¬μΈνΈ"""
    return {
        "service": "Embedding Service",
        "model": MODEL_NAME,
        "status": "running",
        "endpoints": {
            "health": "/health",
            "embed": "/embed",
            "embed_batch": "/embed/batch"
        }
    }
@app.post("/echo")
async def echo(request: Request):
    """POST κ²½λ΅ μ§„λ‹¨μ© μ—μ½” μ—”λ“ν¬μΈνΈ"""
    try:
        body = await request.body()
        logger.info(f"/echo received bytes={len(body)}")
        return {"ok": True, "bytes": len(body)}
    except Exception as e:
        logger.error(f"/echo error: {e}")
        raise HTTPException(status_code=500, detail=str(e))



@app.get("/health")
async def health_check():
    """Health check"""
    return {
        "status": "healthy",
        "model": MODEL_NAME,
        "model_loaded": model is not None
    }


@app.post("/embed", response_model=EmbeddingResponse)
async def generate_embedding(request: EmbeddingRequest):
    """
    λ‹¨μΌ ν…μ¤νΈ μ„λ² λ”© μƒμ„±
    """
    try:
        logger.info(f"/embed called, text_len={len(request.text) if request.text else 0}")
        if not request.text or not request.text.strip():
            raise HTTPException(status_code=400, detail="Text cannot be empty")
        
        # λ¨λΈ λ΅λ“
        m = get_model()
        
        # μ„λ² λ”© μƒμ„±
        embedding = m.encode(
            request.text,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        logger.info("/embed success")
        
        return EmbeddingResponse(
            embedding=embedding.tolist(),
            dimension=len(embedding)
        )
        
    except Exception as e:
        logger.error(f"Error generating embedding: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/embed/batch", response_model=BatchEmbeddingResponse)
async def generate_embeddings_batch(request: BatchEmbeddingRequest):
    """
    λ°°μΉ ν…μ¤νΈ μ„λ² λ”© μƒμ„±
    """
    try:
        logger.info(f"/embed/batch called, count={len(request.texts) if request.texts else 0}")
        if not request.texts:
            raise HTTPException(status_code=400, detail="Texts cannot be empty")
        
        # λ¨λΈ λ΅λ“
        m = get_model()
        
        # λ°°μΉ μ„λ² λ”© μƒμ„±
        embeddings = m.encode(
            request.texts,
            batch_size=32,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        logger.info("/embed/batch success")
        
        return BatchEmbeddingResponse(
            embeddings=embeddings.tolist(),
            count=len(embeddings),
            dimension=len(embeddings[0])
        )
        
    except Exception as e:
        logger.error(f"Error generating batch embeddings: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/similarity")
async def calculate_similarity(text1: str, text2: str):
    """
    λ‘ ν…μ¤νΈ κ°„μ μ μ‚¬λ„ κ³„μ‚°
    """
    try:
        m = get_model()
        
        embeddings = m.encode([text1, text2], normalize_embeddings=True)
        
        # μ½”μ‚¬μΈ μ μ‚¬λ„
        similarity = float(np.dot(embeddings[0], embeddings[1]))
        
        return {
            "text1": text1,
            "text2": text2,
            "similarity": similarity
        }
        
    except Exception as e:
        logger.error(f"Error calculating similarity: {e}")
        raise HTTPException(status_code=500, detail=str(e))

