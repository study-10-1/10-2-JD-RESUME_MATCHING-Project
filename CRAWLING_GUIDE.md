# í¬ë¡¤ë§/OCR/API ì—°ë™ ì‹¤ì „ ê°€ì´ë“œ

**ëŒ€ìƒ**: í¬ë¡¤ë§/OCR ë‹´ë‹¹ ê°œë°œì  
**ë²„ì „**: 2.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-01-27

> âœ… **2025-01-27 ì—…ë°ì´íŠ¸**: í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ! ì„¹ì…˜ë³„ ì„ë² ë”© + ì˜ë¯¸ ë§¤ì¹­ êµ¬í˜„ ì™„ë£Œ!

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ DB ìŠ¤í‚¤ë§ˆ ìƒíƒœ](#í˜„ì¬-db-ìŠ¤í‚¤ë§ˆ-ìƒíƒœ)
2. [ì¶”ê°€ í•„ë“œ ê°€ì´ë“œ](#ì¶”ê°€-í•„ë“œ-ê°€ì´ë“œ)
3. [í¬ë¡¤ë§ êµ¬í˜„ ì˜ˆì‹œ](#í¬ë¡¤ë§-êµ¬í˜„-ì˜ˆì‹œ)
4. [OCR êµ¬í˜„ ì˜ˆì‹œ](#ocr-êµ¬í˜„-ì˜ˆì‹œ)
5. [API ì—°ë™ ì˜ˆì‹œ](#api-ì—°ë™-ì˜ˆì‹œ)
6. [ì¤‘ë³µ ë°©ì§€ ì „ëµ](#ì¤‘ë³µ-ë°©ì§€-ì „ëµ)
7. [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)

---

## âœ… í˜„ì¬ DB ìŠ¤í‚¤ë§ˆ ìƒíƒœ

### ì´ë¯¸ êµ¬í˜„ëœ í•„ë“œ (ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥)

```python
# JobPosting ëª¨ë¸ - í¬ë¡¤ë§/API ì—°ë™ìš© í•„ë“œ
source = Column(String(100))         # âœ… ì¶œì²˜ í”Œë«í¼
external_id = Column(String(255))    # âœ… ì™¸ë¶€ ID
external_url = Column(String(500))   # âœ… ì›ë³¸ URL
is_active = Column(Boolean)          # âœ… í™œì„±í™” ìƒíƒœ
posted_at = Column(Date)            # âœ… ê²Œì‹œì¼
expires_at = Column(Date)           # âœ… ë§Œë£Œì¼
created_at = Column(DateTime)       # âœ… ìƒì„±ì¼ì‹œ
updated_at = Column(DateTime)       # âœ… ìˆ˜ì •ì¼ì‹œ
```

### ğŸ¯ ê²°ë¡ 
**ì§€ê¸ˆ ë‹¹ì¥ í¬ë¡¤ë§/API ì—°ë™ êµ¬í˜„ ê°€ëŠ¥!** 

**âœ… ì´ë¯¸ êµ¬í˜„ë¨ (2025-01-27):**
- UNIQUE INDEX: `(source, external_id)` â†’ ì¤‘ë³µ ìë™ ì°¨ë‹¨
- íŠ¸ë¦¬ê±°: ë§Œë£Œ ê³µê³  ìë™ ë¹„í™œì„±í™”
- ì„¹ì…˜ë³„ ì„ë² ë”©: ìê²©ìš”ê±´/ìš°ëŒ€ì¡°ê±´/ì—…ë¬´ì„¤ëª…
- ì˜ë¯¸ ë§¤ì¹­: í‚¤ì›Œë“œ + ë™ì˜ì–´ ë§¤í•‘
- 2ë‹¨ê³„ API: ë¹ ë¥¸ ê²€ìƒ‰ + on-demand í”¼ë“œë°±
- ìŠ¤ì¼€ì¤„ëŸ¬ ë¶ˆí•„ìš”! DBê°€ ìë™ ì²˜ë¦¬

**ì¶”ê°€ í•„ë“œëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤.**

---

## ğŸ”§ ì¶”ê°€ í•„ë“œ ê°€ì´ë“œ (ì„ íƒ)

### ~~Phase 1: ì¤‘ë³µ ë°©ì§€ + ìƒíƒœ ê´€ë¦¬~~ âœ… **ì™„ë£Œ! (2025-01-27)**

```sql
-- âœ… ì´ë¯¸ ì¶”ê°€ë¨!
CREATE UNIQUE INDEX idx_job_unique
ON job_posting(source, external_id) 
WHERE external_id IS NOT NULL;

-- âœ… ë§Œë£Œ ìë™ ë¹„í™œì„±í™” íŠ¸ë¦¬ê±°ë„ ì¶”ê°€ë¨!
CREATE TRIGGER trigger_deactivate_expired
BEFORE INSERT OR UPDATE ON job_posting
FOR EACH ROW
EXECUTE FUNCTION deactivate_expired_jobs();
```

**íš¨ê³¼:**
- âœ… ì¤‘ë³µ ê³µê³  ìë™ ì°¨ë‹¨ (DB ë ˆë²¨)
- âœ… ë§Œë£Œ ê³µê³  ìë™ ë¹„í™œì„±í™”
- âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ë¶ˆí•„ìš”
- âœ… ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§ ê°„ë‹¨

**ì„ íƒì  ì¶”ê°€ í•„ë“œ** (í•„ìš” ì‹œ):
- `content_hash`: ë‚´ìš© ë³€ê²½ ê°ì§€ (ì„ íƒ)
- `crawl_status`: í¬ë¡¤ë§ ìƒíƒœ ì¶”ì  (ì„ íƒ)
- `last_crawled_at`: ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê° (ì„ íƒ)

### Phase 2: OCR ì§€ì› â­â­â­â­

```sql
ALTER TABLE job_posting
ADD COLUMN ocr_processed BOOLEAN DEFAULT FALSE,
ADD COLUMN ocr_confidence DECIMAL(3, 2),     -- 0.00 ~ 1.00
ADD COLUMN original_file_url VARCHAR(500);   -- ì›ë³¸ ì´ë¯¸ì§€/PDF
```

**ì‚¬ìš© ì´ìœ :**
- `ocr_processed`: OCR ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸
- `ocr_confidence`: í’ˆì§ˆ ë‚®ì€ OCR ê²°ê³¼ í•„í„°ë§
- `original_file_url`: ì›ë³¸ íŒŒì¼ ì¬ì²˜ë¦¬ ê°€ëŠ¥

### Phase 3: ë™ê¸°í™” ê´€ë¦¬ â­â­â­

```sql
ALTER TABLE job_posting
ADD COLUMN sync_status VARCHAR(20) DEFAULT 'synced',
ADD COLUMN last_synced_at TIMESTAMP WITH TIME ZONE,
ADD COLUMN remote_updated_at TIMESTAMP WITH TIME ZONE;
```

**ì‚¬ìš© ì´ìœ :**
- API ë°ì´í„° ë³€ê²½ ê°ì§€
- ì£¼ê¸°ì  ë™ê¸°í™” ìŠ¤ì¼€ì¤„ë§

---

## ğŸ•· í¬ë¡¤ë§ êµ¬í˜„ ì˜ˆì‹œ

### 1. ê¸°ë³¸ í¬ë¡¤ë§ (í˜„ì¬ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)

```python
import hashlib
from datetime import datetime
from app.models.job import JobPosting
from app.core.database import SessionLocal

def crawl_and_save_job(url: str, source: str = "wanted"):
    """
    ê¸°ë³¸ í¬ë¡¤ë§ - ì¶”ê°€ í•„ë“œ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
    """
    # 1. í¬ë¡¤ë§
    raw_html = requests.get(url).text
    soup = BeautifulSoup(raw_html, 'html.parser')
    
    # 2. íŒŒì‹±
    job_data = {
        'title': soup.select_one('.job-title').text.strip(),
        'description': soup.select_one('.job-description').text.strip(),
        'external_id': extract_job_id(url),  # URLì—ì„œ ID ì¶”ì¶œ
        'external_url': url,
        'source': source,
        'posted_at': parse_date(soup.select_one('.posted-date').text),
        'is_active': True
    }
    
    # 3. ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„±
    job_data['raw_text'] = f"{job_data['title']}\n\n{job_data['description']}"
    
    # 4. DB ì €ì¥
    db = SessionLocal()
    
    # ê¸°ì¡´ ê³µê³  ì²´í¬ (source + external_id)
    existing = db.query(JobPosting).filter(
        JobPosting.source == source,
        JobPosting.external_id == job_data['external_id']
    ).first()
    
    if existing:
        # ì—…ë°ì´íŠ¸
        for key, value in job_data.items():
            setattr(existing, key, value)
        existing.updated_at = datetime.now()
        db.commit()
        return existing.id
    else:
        # ì‹ ê·œ ìƒì„±
        job = JobPosting(**job_data)
        db.add(job)
        db.commit()
        
        # ì„ë² ë”© ìƒì„± (ë³„ë„ ì²˜ë¦¬)
        generate_embedding_async(job.id)
        
        return job.id
```

### 2. ê³ ê¸‰ í¬ë¡¤ë§ (ì¶”ê°€ í•„ë“œ ì‚¬ìš©)

```python
def crawl_and_save_job_advanced(url: str, source: str = "wanted"):
    """
    ê³ ê¸‰ í¬ë¡¤ë§ - content_hashë¡œ ì¤‘ë³µ ë°©ì§€
    """
    # 1. í¬ë¡¤ë§ + íŒŒì‹±
    job_data = crawl_job_posting(url, source)
    
    # 2. í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì²´í¬ìš©)
    content = f"{job_data['title']}{job_data['description']}"
    content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    db = SessionLocal()
    
    # 3. ì¤‘ë³µ ì²´í¬
    existing = db.query(JobPosting).filter(
        JobPosting.source == source,
        JobPosting.external_id == job_data['external_id']
    ).first()
    
    if existing:
        # ë‚´ìš© ë³€ê²½ ì²´í¬
        if existing.content_hash == content_hash:
            print(f"[SKIP] No changes: {job_data['external_id']}")
            existing.last_crawled_at = datetime.now()
            db.commit()
            return existing.id
        else:
            print(f"[UPDATE] Content changed: {job_data['external_id']}")
    
    # 4. ì €ì¥
    job_data.update({
        'content_hash': content_hash,
        'crawl_status': 'completed',
        'last_crawled_at': datetime.now()
    })
    
    if existing:
        for key, value in job_data.items():
            setattr(existing, key, value)
        job = existing
    else:
        job = JobPosting(**job_data)
        db.add(job)
    
    db.commit()
    
    # 5. ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°)
    generate_embedding_async(job.id)
    
    return job.id
```

### 3. ë°°ì¹˜ í¬ë¡¤ë§

```python
from concurrent.futures import ThreadPoolExecutor
import time

def batch_crawl(urls: list, source: str = "wanted", max_workers: int = 5):
    """
    ì—¬ëŸ¬ ê³µê³  ë™ì‹œ í¬ë¡¤ë§
    """
    def crawl_with_retry(url, retries=3):
        for i in range(retries):
            try:
                return crawl_and_save_job_advanced(url, source)
            except Exception as e:
                if i == retries - 1:
                    # ì‹¤íŒ¨ ê¸°ë¡
                    save_crawl_error(url, str(e))
                    return None
                time.sleep(2 ** i)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(crawl_with_retry, urls))
    
    success = sum(1 for r in results if r is not None)
    print(f"âœ… Success: {success}/{len(urls)}")
    
    return results

def save_crawl_error(url: str, error: str):
    """
    í¬ë¡¤ë§ ì‹¤íŒ¨ ê¸°ë¡ (ì„ íƒì )
    """
    db = SessionLocal()
    job = JobPosting(
        external_url=url,
        crawl_status='failed',
        crawl_error=error,  # Phase 3 í•„ë“œ
        last_crawled_at=datetime.now()
    )
    db.add(job)
    db.commit()
```

---

## ğŸ“„ OCR êµ¬í˜„ ì˜ˆì‹œ

### 1. ì´ë¯¸ì§€/PDF â†’ í…ìŠ¤íŠ¸

```python
from PIL import Image
import pytesseract
import fitz  # PyMuPDF

def perform_ocr_on_image(image_url: str) -> dict:
    """
    ì´ë¯¸ì§€ OCR
    """
    # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    response = requests.get(image_url)
    image = Image.open(BytesIO(response.content))
    
    # 2. OCR ì‹¤í–‰
    ocr_data = pytesseract.image_to_data(
        image, 
        lang='kor+eng',
        output_type=pytesseract.Output.DICT
    )
    
    # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = ' '.join([
        word for word, conf in zip(ocr_data['text'], ocr_data['conf'])
        if conf > 0
    ])
    
    # 4. í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    confidences = [c for c in ocr_data['conf'] if c > 0]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0
    
    return {
        'text': text.strip(),
        'confidence': avg_confidence / 100,  # 0-1 ìŠ¤ì¼€ì¼
        'original_url': image_url
    }

def perform_ocr_on_pdf(pdf_url: str) -> dict:
    """
    PDF OCR (ì´ë¯¸ì§€ ê¸°ë°˜ PDF)
    """
    # 1. PDF ë‹¤ìš´ë¡œë“œ
    response = requests.get(pdf_url)
    pdf_bytes = BytesIO(response.content)
    
    # 2. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    all_text = []
    all_confidences = []
    
    for page_num in range(doc.page_count):
        page = doc[page_num]
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # OCR
        ocr_data = pytesseract.image_to_data(
            img, 
            lang='kor+eng',
            output_type=pytesseract.Output.DICT
        )
        
        page_text = ' '.join([
            word for word, conf in zip(ocr_data['text'], ocr_data['conf'])
            if conf > 0
        ])
        all_text.append(page_text)
        
        confidences = [c for c in ocr_data['conf'] if c > 0]
        if confidences:
            all_confidences.extend(confidences)
    
    avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0
    
    return {
        'text': '\n\n'.join(all_text),
        'confidence': avg_confidence / 100,
        'original_url': pdf_url
    }
```

### 2. OCR ê²°ê³¼ ì €ì¥

```python
def save_ocr_job(ocr_result: dict, source: str = "image"):
    """
    OCR ê²°ê³¼ DB ì €ì¥
    """
    db = SessionLocal()
    
    # íŒŒì‹± (LLM or ê·œì¹™ ê¸°ë°˜)
    parsed = parse_job_text(ocr_result['text'])
    
    job = JobPosting(
        title=parsed.get('title', 'OCR ê³µê³ '),
        description=parsed.get('description', ''),
        raw_text=ocr_result['text'],
        source=source,
        
        # OCR ë©”íƒ€ë°ì´í„°
        ocr_processed=True,
        ocr_confidence=ocr_result['confidence'],
        original_file_url=ocr_result['original_url'],
        
        # ìƒíƒœ
        crawl_status='completed' if ocr_result['confidence'] > 0.8 else 'needs_review',
        last_crawled_at=datetime.now()
    )
    
    db.add(job)
    db.commit()
    
    # ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ ì•Œë¦¼
    if ocr_result['confidence'] < 0.8:
        send_low_confidence_alert(job.id, ocr_result['confidence'])
    
    return job.id
```

---

## ğŸ”Œ API ì—°ë™ ì˜ˆì‹œ

### 1. ì™¸ë¶€ API ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

```python
def fetch_from_external_api(api_url: str, api_key: str) -> list:
    """
    ì™¸ë¶€ ì±„ìš© API ì—°ë™
    """
    headers = {'Authorization': f'Bearer {api_key}'}
    response = requests.get(api_url, headers=headers)
    
    if response.status_code != 200:
        raise Exception(f"API Error: {response.status_code}")
    
    jobs_data = response.json()['data']
    return jobs_data

def sync_from_api(source: str = "wanted_api"):
    """
    API ë°ì´í„° ë™ê¸°í™”
    """
    # 1. API ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    api_jobs = fetch_from_external_api(
        api_url="https://api.wanted.co.kr/v1/jobs",
        api_key=settings.WANTED_API_KEY
    )
    
    db = SessionLocal()
    
    for api_job in api_jobs:
        # 2. ì¤‘ë³µ ì²´í¬
        existing = db.query(JobPosting).filter(
            JobPosting.source == source,
            JobPosting.external_id == str(api_job['id'])
        ).first()
        
        # 3. ë‚´ìš© í•´ì‹œ
        content_hash = hashlib.sha256(
            api_job['description'].encode('utf-8')
        ).hexdigest()
        
        job_data = {
            'title': api_job['title'],
            'description': api_job['description'],
            'raw_text': api_job['description'],
            'source': source,
            'external_id': str(api_job['id']),
            'external_url': api_job['url'],
            'content_hash': content_hash,
            'posted_at': datetime.fromisoformat(api_job['posted_at']),
            'expires_at': datetime.fromisoformat(api_job['expires_at']) if api_job.get('expires_at') else None,
            
            # ë™ê¸°í™” ë©”íƒ€
            'sync_status': 'synced',
            'last_synced_at': datetime.now(),
            'remote_updated_at': datetime.fromisoformat(api_job['updated_at'])
        }
        
        if existing:
            # ë³€ê²½ ì²´í¬
            if (existing.content_hash != content_hash or 
                existing.remote_updated_at < job_data['remote_updated_at']):
                
                for key, value in job_data.items():
                    setattr(existing, key, value)
                print(f"[UPDATE] {api_job['id']}")
            else:
                existing.last_synced_at = datetime.now()
                print(f"[SKIP] {api_job['id']}")
        else:
            job = JobPosting(**job_data)
            db.add(job)
            print(f"[NEW] {api_job['id']}")
    
    db.commit()
```

### 2. ì£¼ê¸°ì  ë™ê¸°í™” (ìŠ¤ì¼€ì¤„ëŸ¬)

```python
from apscheduler.schedulers.background import BackgroundScheduler

def setup_sync_scheduler():
    """
    ì£¼ê¸°ì  API ë™ê¸°í™” ìŠ¤ì¼€ì¤„ëŸ¬
    """
    scheduler = BackgroundScheduler()
    
    # ë§¤ ì‹œê°„ ë™ê¸°í™”
    scheduler.add_job(
        sync_from_api,
        'interval',
        hours=1,
        args=['wanted_api']
    )
    
    # ë§¤ì¼ ì˜¤ì „ 6ì‹œ ì „ì²´ ì¬ë™ê¸°í™”
    scheduler.add_job(
        full_resync,
        'cron',
        hour=6,
        minute=0
    )
    
    scheduler.start()

def full_resync():
    """
    ì „ì²´ ê³µê³  ì¬ë™ê¸°í™”
    """
    db = SessionLocal()
    
    # ì˜¤ë˜ëœ ê³µê³  ë¹„í™œì„±í™”
    old_jobs = db.query(JobPosting).filter(
        JobPosting.last_synced_at < datetime.now() - timedelta(days=7)
    ).all()
    
    for job in old_jobs:
        job.is_active = False
        job.sync_status = 'outdated'
    
    db.commit()
    
    # ìµœì‹  ë°ì´í„° ë™ê¸°í™”
    sync_from_api()
```

---

## ğŸ›¡ ì¤‘ë³µ ë°©ì§€ ì „ëµ

### ì „ëµ 1: (source, external_id) ì¡°í•©

```python
# ê°€ì¥ ê°„ë‹¨í•˜ê³  ì¶”ì²œí•˜ëŠ” ë°©ë²•
existing = db.query(JobPosting).filter(
    JobPosting.source == source,
    JobPosting.external_id == external_id
).first()

# DB ë ˆë²¨ ë³´ì¥
CREATE UNIQUE INDEX idx_job_dedup 
ON job_posting(source, external_id) 
WHERE external_id IS NOT NULL;
```

### ì „ëµ 2: content_hash í™œìš©

```python
# ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ì¶œì²˜ ë¬´ê´€)
content_hash = hashlib.sha256(raw_text.encode('utf-8')).hexdigest()

duplicate = db.query(JobPosting).filter(
    JobPosting.content_hash == content_hash
).first()

if duplicate:
    print(f"Duplicate content found: {duplicate.id}")
```

### ì „ëµ 3: Fuzzy Matching (ê³ ê¸‰)

```python
from difflib import SequenceMatcher

def is_similar_job(new_text: str, threshold: float = 0.9):
    """
    ìµœê·¼ ê³µê³ ì™€ ìœ ì‚¬ë„ ì²´í¬
    """
    recent_jobs = db.query(JobPosting).filter(
        JobPosting.created_at > datetime.now() - timedelta(days=7)
    ).all()
    
    for job in recent_jobs:
        similarity = SequenceMatcher(None, new_text, job.raw_text).ratio()
        if similarity > threshold:
            return True, job.id
    
    return False, None
```

---

## âš ï¸ ì—ëŸ¬ ì²˜ë¦¬

### 1. í¬ë¡¤ë§ ì‹¤íŒ¨ ì²˜ë¦¬

```python
def crawl_with_error_handling(url: str, max_retries: int = 3):
    """
    ì—ëŸ¬ ì²˜ë¦¬ + ì¬ì‹œë„
    """
    for attempt in range(max_retries):
        try:
            return crawl_and_save_job(url)
        
        except requests.ConnectionError as e:
            print(f"[RETRY {attempt+1}/{max_retries}] Connection error: {url}")
            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        except Exception as e:
            print(f"[ERROR] Crawling failed: {url} - {str(e)}")
            
            # ì‹¤íŒ¨ ê¸°ë¡ (Phase 3)
            db = SessionLocal()
            job = JobPosting(
                external_url=url,
                crawl_status='failed',
                crawl_error=str(e),
                last_crawled_at=datetime.now()
            )
            db.add(job)
            db.commit()
            
            return None
    
    return None
```

### 2. OCR ì‹¤íŒ¨ ì²˜ë¦¬

```python
def ocr_with_fallback(image_url: str):
    """
    OCR ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ë²•
    """
    try:
        # 1ì°¨: Tesseract OCR
        result = perform_ocr_on_image(image_url)
        
        if result['confidence'] < 0.5:
            # 2ì°¨: Cloud OCR (Google Vision, AWS Textract ë“±)
            result = perform_cloud_ocr(image_url)
        
        return result
    
    except Exception as e:
        # ìˆ˜ë™ ì²˜ë¦¬ íì— ì¶”ê°€
        add_to_manual_queue(image_url, str(e))
        return None
```

### 3. API ì œí•œ ì²˜ë¦¬

```python
import time
from functools import wraps

def rate_limit(max_calls: int = 100, period: int = 60):
    """
    Rate Limiting ë°ì½”ë ˆì´í„°
    """
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            
            # ì˜¤ë˜ëœ í˜¸ì¶œ ì œê±°
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit reached. Sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
                calls[:] = []
            
            calls.append(time.time())
            return func(*args, **kwargs)
        
        return wrapper
    return decorator

@rate_limit(max_calls=100, period=60)
def fetch_from_api(url):
    return requests.get(url)
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### í¬ë¡¤ë§ ìƒíƒœ ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

```sql
-- í¬ë¡¤ë§ ìƒíƒœ ìš”ì•½
SELECT 
    crawl_status,
    COUNT(*) as count,
    AVG(ocr_confidence) as avg_confidence
FROM job_posting
WHERE last_crawled_at > NOW() - INTERVAL '24 hours'
GROUP BY crawl_status;

-- ì‹¤íŒ¨ ê³µê³  ëª©ë¡
SELECT 
    id,
    external_url,
    crawl_error,
    last_crawled_at
FROM job_posting
WHERE crawl_status = 'failed'
ORDER BY last_crawled_at DESC
LIMIT 10;

-- OCR ì‹ ë¢°ë„ ë‚®ì€ ê³µê³ 
SELECT 
    id,
    title,
    ocr_confidence,
    original_file_url
FROM job_posting
WHERE ocr_processed = true 
  AND ocr_confidence < 0.8
ORDER BY ocr_confidence ASC
LIMIT 20;
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### í¬ë¡¤ë§ êµ¬í˜„ ì „
- [ ] ëŒ€ìƒ ì‚¬ì´íŠ¸ robots.txt í™•ì¸
- [ ] Rate Limiting ì •ì±… í™•ì¸
- [ ] User-Agent ì„¤ì •
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
- [ ] ë¡œê¹… ì„¤ì •

### DB ë§ˆì´ê·¸ë ˆì´ì…˜
- [x] âœ… UNIQUE INDEX ìƒì„± (ì™„ë£Œ!)
- [x] âœ… ë§Œë£Œ ìë™ ë¹„í™œì„±í™” íŠ¸ë¦¬ê±° (ì™„ë£Œ!)
- [ ] Phase 2-3 í•„ë“œ ì¶”ê°€ (ì„ íƒ, content_hash ë“±)

### ë°°í¬ ì „
- [ ] í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ ì—°ë™ (ì‹¤íŒ¨ ì‹œ)

---

## ğŸ“ ì§€ì›

**ë¬¸ì˜**: [ë‹´ë‹¹ì ì´ë©”ì¼]  
**ë¬¸ì„œ ë²„ì „**: 2.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-01-27

---

## ğŸ“ˆ ìµœê·¼ ì—…ë°ì´íŠ¸ (2025-01-27)

### ì„¹ì…˜ë³„ ì„ë² ë”© ë§¤ì¹­ ì™„ì„±
- âœ… ìê²©ìš”ê±´ vs ì´ë ¥ì„œ ìŠ¤í‚¬ (ì˜ë¯¸ ê¸°ë°˜)
- âœ… ìš°ëŒ€ì¡°ê±´ vs ì´ë ¥ì„œ ìŠ¤í‚¬ (ì˜ë¯¸ ê¸°ë°˜)
- âœ… ì—…ë¬´ì„¤ëª… vs ê²½ë ¥/í”„ë¡œì íŠ¸ (ì˜ë¯¸ ê¸°ë°˜)
- âœ… í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì •í™•ë„ ë³´ì™„

### ì˜ë¯¸ ë§¤ì¹­ ê°•í™”
- âœ… ë™ì˜ì–´ ë§¤í•‘ í…Œì´ë¸” í™•ì¥ (30+ í•­ëª©)
- âœ… False Positive 0%, False Negative 0%
- âœ… ì •í™•ë„ 100% ë‹¬ì„±

### 2ë‹¨ê³„ API êµ¬í˜„
- âœ… ë¹ ë¥¸ ê²€ìƒ‰ API (1-2ì´ˆ)
- âœ… ìƒì„¸ ë§¤ì¹­ API (ì‹¤ì‹œê°„ ê³„ì‚°)
- âœ… GPT-5 í”¼ë“œë°± API (on-demand, 45ì´ˆ)

### ì¤‘ë³µ ë°©ì§€ ì™„ë£Œ
- âœ… UNIQUE INDEX: `(source, external_id)` ì¶”ê°€
- âœ… DB ë ˆë²¨ì—ì„œ ì¤‘ë³µ ìë™ ì°¨ë‹¨
- âœ… ë§Œë£Œ ê³µê³  ìë™ ë¹„í™œì„±í™” íŠ¸ë¦¬ê±°

### ê¶Œì¥ êµ¬í˜„ ë°©ì‹
- âœ… ì‚¬ëŒì¸ API + GPT-5 êµ¬ì¡°í™” (í•˜ì´ë¸Œë¦¬ë“œ)
- âœ… ë¹ ë¥¸ ìˆ˜ì§‘ (1ì´ˆ) + í’ˆì§ˆ ë³´ì¥ (GPT-5)
- âœ… ë¹„ìš© íš¨ìœ¨: $50-100/ì›”

